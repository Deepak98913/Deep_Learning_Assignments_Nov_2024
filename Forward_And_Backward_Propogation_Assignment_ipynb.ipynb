{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Explain the concept of forward propagation in a neural network ?\n",
        "\n",
        "Ans:- Forward propagation is the process of passing input data through a neural network to generate an output. It is a key step in training and inference in neural networks. The process involves the following steps:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Input Layer**\n",
        "- The input data is provided to the network through the input layer.\n",
        "- Each input feature corresponds to a neuron in the input layer.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Weighted Sum Calculation**\n",
        "- In each neuron of the subsequent layers, a **weighted sum** of the inputs is calculated:\n",
        "  \n",
        "  \\[\n",
        "  z = \\sum_{i=1}^{n} w_i x_i + b\n",
        "  \\]\n",
        "  \n",
        "  - \\(x_i\\): Input values.\n",
        "  - \\(w_i\\): Weights associated with the inputs.\n",
        "  - \\(b\\): Bias term.\n",
        "  \n",
        "  This operation determines the neuron’s raw activation.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Activation Function**\n",
        "- The raw activation (\\(z\\)) is passed through an **activation function** to introduce non-linearity:\n",
        "  \n",
        "  \\[\n",
        "  a = \\sigma(z)\n",
        "  \\]\n",
        "  \n",
        "  - \\(\\sigma\\): Activation function (e.g., ReLU, Sigmoid, Tanh).\n",
        "  - \\(a\\): Activated value that becomes the output of the neuron.\n",
        "  \n",
        "  Non-linear activation functions enable the network to model complex relationships in the data.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Layer-by-Layer Propagation**\n",
        "- The activated outputs (\\(a\\)) of one layer serve as inputs (\\(x\\)) to the next layer.\n",
        "- This process is repeated for all layers in the network until the final layer (output layer).\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Output Layer**\n",
        "- In the output layer, the final predictions are made.\n",
        "  - If it’s a classification task, the output might be probabilities (e.g., using the softmax function).\n",
        "  - If it’s a regression task, the output could be a single continuous value.\n",
        "\n",
        "---\n",
        "\n",
        "### Example of Forward Propagation\n",
        "For a simple neural network with two inputs, one hidden layer, and one output neuron:\n",
        "1. **Inputs**: \\(x_1\\) and \\(x_2\\)\n",
        "2. **Hidden Layer**:\n",
        "   - Neuron 1: \\(z_1 = w_{11}x_1 + w_{12}x_2 + b_1\\), \\(a_1 = \\sigma(z_1)\\)\n",
        "   - Neuron 2: \\(z_2 = w_{21}x_1 + w_{22}x_2 + b_2\\), \\(a_2 = \\sigma(z_2)\\)\n",
        "3. **Output Layer**:\n",
        "   - \\(z_{\\text{output}} = w_{31}a_1 + w_{32}a_2 + b_3\\)\n",
        "   - Final output: \\(y_{\\text{pred}} = \\sigma(z_{\\text{output}})\\)\n"
      ],
      "metadata": {
        "id": "j7KDeFO63-bX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. What is the purpose of the activation function in forward propagation ?\n",
        "Ans :- The **activation function** plays a critical role in forward propagation by introducing **non-linearity** into the neural network. Without it, the network would behave like a simple linear model, regardless of the number of layers. Here's why the activation function is essential:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Non-Linear Modeling**\n",
        "- Real-world data often involves complex, non-linear relationships.\n",
        "- Activation functions allow the network to model and learn these non-linear patterns effectively.\n",
        "- Without them, the network would only be able to model linear relationships, limiting its expressiveness and utility.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Introducing Hierarchical Representations**\n",
        "- Activation functions enable the network to build hierarchical and abstract representations of the input data.\n",
        "- For example, in image recognition, earlier layers might detect edges, while deeper layers recognize objects or patterns.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Controlling Signal Flow**\n",
        "- Activation functions determine whether a neuron \"fires\" (i.e., produces a significant output).\n",
        "- Functions like ReLU ensure that only neurons with meaningful outputs contribute to the next layer, promoting sparsity and efficiency.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Avoiding Vanishing or Exploding Gradients**\n",
        "- Carefully chosen activation functions (e.g., ReLU, Leaky ReLU) help mitigate the vanishing gradient problem in deep networks, ensuring gradients remain large enough for effective learning during backpropagation.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Output Transformation**\n",
        "- In the output layer, specific activation functions are used based on the task:\n",
        "  - **Softmax**: Converts raw scores into probabilities for multi-class classification.\n",
        "  - **Sigmoid**: Maps outputs to the range (0, 1), useful for binary classification.\n",
        "  - **Linear**: Used for regression tasks where the output is continuous.\n",
        "\n",
        "---\n",
        "\n",
        "### Example: Why Non-Linearity is Crucial\n",
        "Consider a neural network without activation functions:\n",
        "- Each layer performs a linear transformation (\\(Wx + b\\)).\n",
        "- Stacking multiple layers results in another linear transformation (\\(W' x + b'\\)), regardless of the depth.\n",
        "  \n",
        "Adding activation functions introduces non-linearity, enabling the composition of functions that approximate any complex relationship.\n",
        "\n",
        "---\n",
        "\n",
        "### Common Activation Functions\n",
        "1. **ReLU (Rectified Linear Unit)**:\n",
        "   - \\(f(x) = \\max(0, x)\\)\n",
        "   - Widely used due to simplicity and efficiency.\n",
        "   \n",
        "2. **Sigmoid**:\n",
        "   - \\(f(x) = \\frac{1}{1 + e^{-x}}\\)\n",
        "   - Outputs in the range (0, 1), suitable for probabilities.\n",
        "\n",
        "3. **Tanh**:\n",
        "   - \\(f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\)\n",
        "   - Outputs in the range (-1, 1), centered around zero.\n",
        "\n",
        "4. **Leaky ReLU**:\n",
        "   - \\(f(x) = x\\) if \\(x > 0\\), otherwise \\(f(x) = \\alpha x\\) (\\(\\alpha > 0\\)).\n",
        "   - Solves ReLU’s issue of \"dying neurons.\"\n",
        "\n",
        "5. **Softmax**:\n",
        "   - \\(f(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\\)\n",
        "   - Converts logits to a probability distribution.\n"
      ],
      "metadata": {
        "id": "iX0OqEuB4VaF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Describe the steps involved in the backward propagation (backpropagation )algorithm.\n",
        "\n",
        "Ans:-\n",
        "The **backward propagation** (or **backpropagation**) algorithm is a core component of training a neural network. It is used to compute gradients of the loss function with respect to the network's parameters (weights and biases) and updates them to minimize the loss. Here are the detailed steps:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Forward Propagation**\n",
        "- Input data is passed through the network layer by layer, and outputs are computed.\n",
        "- The loss function is evaluated using the predicted output and the true labels.\n",
        "- This step sets up the required values (activations, pre-activations) for backpropagation.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Compute Loss Derivative**\n",
        "- The derivative of the **loss function** with respect to the output layer activations is computed:\n",
        "  \n",
        "  \\[\n",
        "  \\frac{\\partial L}{\\partial a^{(L)}}\n",
        "  \\]\n",
        "  \n",
        "  where \\(L\\) is the loss, and \\(a^{(L)}\\) represents the activations of the output layer.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Backpropagate Through the Output Layer**\n",
        "- Using the chain rule, compute the gradient of the loss with respect to:\n",
        "  1. **Output layer’s pre-activation**:\n",
        "     \\[\n",
        "     \\delta^{(L)} = \\frac{\\partial L}{\\partial z^{(L)}} = \\frac{\\partial L}{\\partial a^{(L)}} \\odot \\sigma'(z^{(L)})\n",
        "     \\]\n",
        "     where \\(\\sigma'(z^{(L)})\\) is the derivative of the activation function.\n",
        "  2. **Weights and biases**:\n",
        "     \\[\n",
        "     \\frac{\\partial L}{\\partial W^{(L)}} = \\delta^{(L)} \\cdot a^{(L-1)}^T\n",
        "     \\]\n",
        "     \\[\n",
        "     \\frac{\\partial L}{\\partial b^{(L)}} = \\delta^{(L)}\n",
        "     \\]\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Backpropagate Through Hidden Layers**\n",
        "- For each hidden layer \\(l\\) (from \\(L-1\\) to 1), compute:\n",
        "  1. **Error term (\\(\\delta\\))**:\n",
        "     \\[\n",
        "     \\delta^{(l)} = (W^{(l+1)^T} \\delta^{(l+1)}) \\odot \\sigma'(z^{(l)})\n",
        "     \\]\n",
        "     This represents how much each neuron in layer \\(l\\) contributed to the error in the output.\n",
        "  2. **Gradients of weights and biases**:\n",
        "     \\[\n",
        "     \\frac{\\partial L}{\\partial W^{(l)}} = \\delta^{(l)} \\cdot a^{(l-1)}^T\n",
        "     \\]\n",
        "     \\[\n",
        "     \\frac{\\partial L}{\\partial b^{(l)}} = \\delta^{(l)}\n",
        "     \\]\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Update Weights and Biases**\n",
        "- Once all gradients are computed, update the weights and biases using an optimization algorithm, such as **gradient descent**:\n",
        "  \\[\n",
        "  W^{(l)} \\gets W^{(l)} - \\eta \\frac{\\partial L}{\\partial W^{(l)}}\n",
        "  \\]\n",
        "  \\[\n",
        "  b^{(l)} \\gets b^{(l)} - \\eta \\frac{\\partial L}{\\partial b^{(l)}}\n",
        "  \\]\n",
        "  - \\(\\eta\\): Learning rate, which controls the step size for updates.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Repeat**\n",
        "- The process of forward propagation, loss computation, and backpropagation is repeated for multiple iterations (epochs) until the model converges to a minimal loss.\n"
      ],
      "metadata": {
        "id": "A2XPGEcR4VWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. What is the purpose of the chain rule in backpropagation.\n",
        "Ans :- The **chain rule** in calculus is essential to the backpropagation algorithm because it allows the calculation of how changes in the weights and biases at each layer of the neural network affect the overall loss. Neural networks are composed of multiple layers, each applying transformations to data. The chain rule helps propagate the error (loss gradient) backward through these layers to update their parameters effectively.\n",
        "\n",
        "---\n",
        "\n",
        "### Purpose of the Chain Rule in Backpropagation\n",
        "\n",
        "1. **Gradient Calculation Across Layers**:\n",
        "   - In a neural network, the output of one layer becomes the input to the next layer. The loss function depends on these transformations.\n",
        "   - To optimize the network, we need the gradients of the loss with respect to each layer's parameters (\\(W, b\\)). The chain rule enables us to compute these gradients step by step.\n",
        "\n",
        "   For a function composition \\(f(g(h(x)))\\), the chain rule states:\n",
        "   \\[\n",
        "   \\frac{d}{dx} f(g(h(x))) = f'(g(h(x))) \\cdot g'(h(x)) \\cdot h'(x)\n",
        "   \\]\n",
        "   This principle is applied layer by layer in a neural network.\n",
        "\n",
        "---\n",
        "\n",
        "2. **Error Propagation (Backpropagation)**:\n",
        "   - The chain rule is used to propagate the error (gradient of the loss) backward from the output layer to the input layer.\n",
        "   - For each layer, the gradient of the loss with respect to its parameters depends on:\n",
        "     - The gradient of the loss with respect to the outputs of that layer.\n",
        "     - The gradient of the layer’s output with respect to its parameters.\n",
        "\n",
        "---\n",
        "\n",
        "3. **Parameter Updates**:\n",
        "   - Backpropagation uses the chain rule to compute the gradients required to update weights (\\(W\\)) and biases (\\(b\\)):\n",
        "     - **Weights**: \\(\\frac{\\partial L}{\\partial W^{(l)}} = \\delta^{(l)} \\cdot a^{(l-1)}^T\\)\n",
        "     - **Biases**: \\(\\frac{\\partial L}{\\partial b^{(l)}} = \\delta^{(l)}\\)\n",
        "\n",
        "   - The chain rule ensures that the updates are influenced by how each parameter indirectly affects the loss through all subsequent layers.\n",
        "\n",
        "---\n",
        "\n",
        "4. **Efficiency with Shared Dependencies**:\n",
        "   - Many neural network computations share intermediate values (e.g., activations, pre-activations). Using the chain rule, backpropagation computes gradients efficiently by reusing these shared computations, reducing redundancy.\n",
        "\n",
        "---\n",
        "\n",
        "### Example in a Neural Network\n",
        "For a network with one hidden layer:\n",
        "1. **Loss**: \\(L\\) depends on the output \\(a^{(2)}\\) of the output layer.\n",
        "2. The output layer's activation depends on the hidden layer's activation (\\(a^{(1)}\\)).\n",
        "3. Using the chain rule, we compute:\n",
        "   \\[\n",
        "   \\frac{\\partial L}{\\partial W^{(1)}} = \\frac{\\partial L}{\\partial a^{(2)}} \\cdot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial a^{(1)}} \\cdot \\frac{\\partial a^{(1)}}{\\partial z^{(1)}} \\cdot \\frac{\\partial z^{(1)}}{\\partial W^{(1)}}\n",
        "   \\]\n",
        "\n",
        "---\n",
        "\n",
        "### Why the Chain Rule is Crucial\n",
        "- **Handles Dependencies**: The chain rule accounts for how each parameter indirectly affects the loss through its impact on downstream layers.\n",
        "- **Facilitates Training**: By breaking down the gradient computation into smaller, manageable pieces, the chain rule makes it feasible to compute gradients for deep networks efficiently.\n",
        "- **Foundation of Backpropagation**: Without the chain rule, propagating errors through the network and updating parameters would be mathematically infeasible.\n"
      ],
      "metadata": {
        "id": "FYjwpW0t4VTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Implement the forward propagation process for a simple neural network with one hidden layer using NumPy.\n",
        "\n",
        "Ans :- Here's an implementation of the **forward propagation** process for a simple neural network with one hidden layer using **NumPy**. We'll assume the following structure:\n",
        "\n",
        "1. Input layer: \\(n_{\\text{inputs}}\\) neurons.\n",
        "2. Hidden layer: \\(n_{\\text{hidden}}\\) neurons.\n",
        "3. Output layer: \\(n_{\\text{outputs}}\\) neurons.\n",
        "\n",
        "We'll use **ReLU** as the activation function for the hidden layer and a **softmax** function for the output layer.\n",
        "\n",
        "---\n",
        "\n",
        "### Implementation Code\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Activation functions\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # For numerical stability\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "# Initialize weights and biases\n",
        "def initialize_parameters(n_inputs, n_hidden, n_outputs):\n",
        "    np.random.seed(42)  # For reproducibility\n",
        "    W1 = np.random.randn(n_inputs, n_hidden) * 0.01  # Weights for input to hidden layer\n",
        "    b1 = np.zeros((1, n_hidden))                    # Biases for hidden layer\n",
        "    W2 = np.random.randn(n_hidden, n_outputs) * 0.01  # Weights for hidden to output layer\n",
        "    b2 = np.zeros((1, n_outputs))                   # Biases for output layer\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "# Forward propagation\n",
        "def forward_propagation(X, W1, b1, W2, b2):\n",
        "    # Hidden layer\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = relu(Z1)  # Activation for hidden layer\n",
        "    \n",
        "    # Output layer\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = softmax(Z2)  # Activation for output layer (softmax for probabilities)\n",
        "    \n",
        "    return A1, A2  # Return hidden layer activations and output layer activations\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example data\n",
        "    X = np.array([[0.5, 1.5, -1.0],   # Input data (3 samples, 3 features each)\n",
        "                  [-1.5, 2.0, 1.0],\n",
        "                  [1.0, -1.0, 0.5]])\n",
        "    n_inputs = X.shape[1]\n",
        "    n_hidden = 4  # Number of neurons in hidden layer\n",
        "    n_outputs = 3  # Number of output neurons (e.g., for 3-class classification)\n",
        "\n",
        "    # Initialize parameters\n",
        "    W1, b1, W2, b2 = initialize_parameters(n_inputs, n_hidden, n_outputs)\n",
        "\n",
        "    # Forward propagation\n",
        "    A1, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
        "\n",
        "    print(\"Hidden layer activations (A1):\\n\", A1)\n",
        "    print(\"\\nOutput layer activations (A2):\\n\", A2)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Explanation\n",
        "\n",
        "1. **Weights and Bias Initialization**:\n",
        "   - Randomly initialize weights \\(W1, W2\\) with small values.\n",
        "   - Set biases \\(b1, b2\\) to zeros.\n",
        "\n",
        "2. **Hidden Layer**:\n",
        "   - Compute pre-activation \\(Z1 = X \\cdot W1 + b1\\).\n",
        "   - Apply **ReLU** activation to \\(Z1\\) to get \\(A1\\) (hidden layer activations).\n",
        "\n",
        "3. **Output Layer**:\n",
        "   - Compute pre-activation \\(Z2 = A1 \\cdot W2 + b2\\).\n",
        "   - Apply **softmax** activation to \\(Z2\\) to get \\(A2\\) (output probabilities).\n",
        "\n",
        "4. **Return Values**:\n",
        "   - \\(A1\\): Hidden layer activations.\n",
        "   - \\(A2\\): Output probabilities.\n",
        "\n",
        "---\n",
        "\n",
        "### Example Output\n",
        "With randomly initialized weights and the given input data \\(X\\), the script will output the activations of the hidden layer (\\(A1\\)) and the output layer (\\(A2\\)). Adjust \\(n_hidden\\) and \\(n_outputs\\) based on your network structure."
      ],
      "metadata": {
        "id": "Rmt0-IXm4VQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JHMunsmq4VNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NKyPwzB14VKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ChVTx85Z4VHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "U4QopocS4VCJ"
      }
    }
  ]
}